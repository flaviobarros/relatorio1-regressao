---
title: "Relatório 1 - Regressão"
author: 
    - Flavio Margarito Martins de Barros 
    - Gabriel Tupinamba da Cunha Leandro 
    - Gustavo Leite Machado
date: "14/05/2022"
output: pdf_document
header-includes:
- \newcommand{\Minimize}{\mathop{\mathrm{Minimize}}\limits}
- \usepackage[brazil]{babel}
- \usepackage{bbm}
- \usepackage{amsmath}
- \usepackage{mathtools}
- \usepackage{mathrsfs}
- \usepackage[makeroom]{cancel}
- \usepackage{xcolor}
- \usepackage{setspace}
- \usepackage{xspace}
- \usepackage{relsize}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r echo=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
## Carregando os pacotes
require(readxl)
require(corrplot)
require(psych)
require(kableExtra)
require(caret)
require(GGally)
require(Hmisc)
```


## Descrição básica dos dados

```{r echo=TRUE, cache=TRUE}
## Lendo o banco de dados
## Fonte: https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength
dados <- read_excel(path = "Concrete_Data.xls", sheet = 1)

## Trocando os nomes das variáveis para o português
colnames(dados) <-
  c(
    "cimento",
    "escoria",
    "cinza",
    "agua",
    "super_plastificante",
    "agregador_grosso",
    "agregador_fino",
    "idade",
    "forca_compressiva"
  )

sum(is.na(data.frame(dados)))


ggplot(dados, aes(x = forca_compressiva)) +
  geom_histogram(aes(y=..density..)) +
  geom_density(alpha=.2, fill="#FF6666") 
```

* Cimento               ($kg/m^3$)
* Escoria               ($kg/m^3$)
* Cinza                 ($kg/m^3$)
* Agua                  ($kg/m^3$)
* Super plastificante   ($kg/m^3$)
* Agregadro grosso      ($kg/m^3$)
* Agregador fino        ($kg/m^3$)
* Idade                          (Dias 1 a 365)
* Força compressiva (Target)     ($MPa$) 

Como podemos notar, temos 1030 observações, 8 variáveis explicativas e nossa variável de interesse (força compressiva), e nenhum dado faltante nas observações. Pela descrição básica dos dados, não temos nenhum dado que parece fugir dos valores esperados (por exemplo: não temos valores negativos).

Analisando a nossa variável resposta podemos notar que sua distribuição se assemelha a uma normal

```{r, echo=TRUE, eval=FALSE}
## Sumario dos dados
d <- Hmisc::describe(dados)
```

\begin{spacing}{0.7}
\begin{center}\textbf{ dados \\ 9 Variables~~~~~ 1030 ~Observations}\end{center}
\smallskip\hrule\smallskip{\small
\vbox{\noindent\textbf{cimento}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){7}}
\put(34,0){\line(0,1){7}}
\put(50,0){\line(0,1){7}}
\put(84,0){\line(0,1){7}}
\put(101,0){\line(0,1){4}}
\put(118,0){\line(0,1){25}}
\put(135,0){\line(0,1){29}}
\put(151,0){\line(0,1){36}}
\put(168,0){\line(0,1){42}}
\put(185,0){\line(0,1){60}}
\put(202,0){\line(0,1){33}}
\put(219,0){\line(0,1){67}}
\put(235,0){\line(0,1){31}}
\put(252,0){\line(0,1){25}}
\put(269,0){\line(0,1){31}}
\put(286,0){\line(0,1){13}}
\put(303,0){\line(0,1){44}}
\put(320,0){\line(0,1){22}}
\put(336,0){\line(0,1){31}}
\put(353,0){\line(0,1){11}}
\put(370,0){\line(0,1){29}}
\put(387,0){\line(0,1){73}}
\put(404,0){\line(0,1){31}}
\put(420,0){\line(0,1){4}}
\put(437,0){\line(0,1){31}}
\put(454,0){\line(0,1){35}}
\put(471,0){\line(0,1){36}}
\put(488,0){\line(0,1){9}}
\put(505,0){\line(0,1){100}}
\put(521,0){\line(0,1){22}}
\put(538,0){\line(0,1){11}}
\put(555,0){\line(0,1){16}}
\put(589,0){\line(0,1){62}}
\put(605,0){\line(0,1){9}}
\put(622,0){\line(0,1){25}}
\put(639,0){\line(0,1){35}}
\put(656,0){\line(0,1){40}}
\put(673,0){\line(0,1){27}}
\put(690,0){\line(0,1){33}}
\put(706,0){\line(0,1){27}}
\put(723,0){\line(0,1){44}}
\put(740,0){\line(0,1){33}}
\put(757,0){\line(0,1){18}}
\put(774,0){\line(0,1){36}}
\put(790,0){\line(0,1){15}}
\put(807,0){\line(0,1){38}}
\put(841,0){\line(0,1){36}}
\put(858,0){\line(0,1){9}}
\put(875,0){\line(0,1){15}}
\put(891,0){\line(0,1){40}}
\put(925,0){\line(0,1){35}}
\put(942,0){\line(0,1){44}}
\put(959,0){\line(0,1){33}}
\put(975,0){\line(0,1){18}}
\put(992,0){\line(0,1){22}}
\put(1009,0){\line(0,1){13}}
\put(1026,0){\line(0,1){2}}
\put(1093,0){\line(0,1){51}}
\put(1110,0){\line(0,1){11}}
\put(1127,0){\line(0,1){2}}
\put(1144,0){\line(0,1){9}}
\put(1160,0){\line(0,1){25}}
\put(1177,0){\line(0,1){4}}
\put(1245,0){\line(0,1){9}}
\put(1261,0){\line(0,1){24}}
\put(1278,0){\line(0,1){13}}
\put(1295,0){\line(0,1){2}}
\put(1312,0){\line(0,1){15}}
\put(1345,0){\line(0,1){20}}
\put(1362,0){\line(0,1){2}}
\put(1396,0){\line(0,1){4}}
\put(1413,0){\line(0,1){7}}
\put(1430,0){\line(0,1){13}}
\put(1446,0){\line(0,1){11}}
\put(1480,0){\line(0,1){16}}
\end{picture}

{\smaller
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&280&1&281.2&118.5&143.7&153.5&192.4&272.9&350.0&425.0&480.0 \end{tabular}
\begin{verbatim}

lowest : 102.0 108.3 116.0 122.6 132.0, highest: 522.0 525.0 528.0 531.3 540.0
\end{verbatim}
}
\smallskip\hrule\smallskip
}
\vbox{\noindent\textbf{escoria}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){100}}
\put(41,0){\line(0,1){1}}
\put(61,0){\line(0,1){2}}
\put(82,0){\line(0,1){7}}
\put(102,0){\line(0,1){5}}
\put(163,0){\line(0,1){3}}
\put(184,0){\line(0,1){1}}
\put(204,0){\line(0,1){3}}
\put(225,0){\line(0,1){2}}
\put(306,0){\line(0,1){2}}
\put(347,0){\line(0,1){1}}
\put(368,0){\line(0,1){1}}
\put(388,0){\line(0,1){6}}
\put(408,0){\line(0,1){5}}
\put(429,0){\line(0,1){5}}
\put(449,0){\line(0,1){1}}
\put(470,0){\line(0,1){4}}
\put(490,0){\line(0,1){3}}
\put(510,0){\line(0,1){1}}
\put(531,0){\line(0,1){5}}
\put(551,0){\line(0,1){3}}
\put(572,0){\line(0,1){3}}
\put(592,0){\line(0,1){5}}
\put(612,0){\line(0,1){2}}
\put(633,0){\line(0,1){3}}
\put(653,0){\line(0,1){2}}
\put(674,0){\line(0,1){3}}
\put(694,0){\line(0,1){2}}
\put(715,0){\line(0,1){3}}
\put(735,0){\line(0,1){1}}
\put(755,0){\line(0,1){3}}
\put(776,0){\line(0,1){10}}
\put(796,0){\line(0,1){1}}
\put(817,0){\line(0,1){2}}
\put(837,0){\line(0,1){2}}
\put(858,0){\line(0,1){4}}
\put(878,0){\line(0,1){1}}
\put(939,0){\line(0,1){1}}
\put(960,0){\line(0,1){2}}
\put(980,0){\line(0,1){2}}
\put(1000,0){\line(0,1){1}}
\put(1021,0){\line(0,1){1}}
\put(1062,0){\line(0,1){1}}
\put(1123,0){\line(0,1){1}}
\put(1164,0){\line(0,1){1}}
\put(1184,0){\line(0,1){1}}
\put(1245,0){\line(0,1){1}}
\put(1286,0){\line(0,1){1}}
\put(1388,0){\line(0,1){1}}
\put(1470,0){\line(0,1){1}}
\end{picture}

{\smaller[2]
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&187&0.907&73.9&91.71&  0.0&  0.0&  0.0& 22.0&142.9&192.0&236.0 \end{tabular}
\begin{verbatim}

lowest :   0.00   0.02  11.00  13.61  15.00, highest: 290.20 305.30 316.10 342.10 359.40
\end{verbatim}
}
\smallskip\hrule\smallskip
}
\vbox{\noindent\textbf{cinza}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){100}}
\put(177,0){\line(0,1){3}}
\put(442,0){\line(0,1){1}}
\put(531,0){\line(0,1){1}}
\put(560,0){\line(0,1){1}}
\put(575,0){\line(0,1){1}}
\put(590,0){\line(0,1){2}}
\put(605,0){\line(0,1){1}}
\put(634,0){\line(0,1){1}}
\put(649,0){\line(0,1){1}}
\put(664,0){\line(0,1){1}}
\put(678,0){\line(0,1){1}}
\put(693,0){\line(0,1){4}}
\put(708,0){\line(0,1){5}}
\put(723,0){\line(0,1){3}}
\put(737,0){\line(0,1){5}}
\put(767,0){\line(0,1){1}}
\put(782,0){\line(0,1){1}}
\put(796,0){\line(0,1){2}}
\put(826,0){\line(0,1){3}}
\put(841,0){\line(0,1){1}}
\put(855,0){\line(0,1){1}}
\put(870,0){\line(0,1){6}}
\put(885,0){\line(0,1){1}}
\put(899,0){\line(0,1){5}}
\put(914,0){\line(0,1){5}}
\put(929,0){\line(0,1){4}}
\put(944,0){\line(0,1){2}}
\put(958,0){\line(0,1){1}}
\put(973,0){\line(0,1){3}}
\put(988,0){\line(0,1){1}}
\put(1003,0){\line(0,1){1}}
\put(1017,0){\line(0,1){1}}
\put(1032,0){\line(0,1){3}}
\put(1047,0){\line(0,1){1}}
\put(1062,0){\line(0,1){2}}
\put(1076,0){\line(0,1){1}}
\put(1091,0){\line(0,1){1}}
\put(1106,0){\line(0,1){1}}
\put(1121,0){\line(0,1){1}}
\put(1165,0){\line(0,1){1}}
\put(1180,0){\line(0,1){1}}
\put(1209,0){\line(0,1){2}}
\put(1224,0){\line(0,1){1}}
\put(1239,0){\line(0,1){2}}
\put(1268,0){\line(0,1){1}}
\put(1283,0){\line(0,1){4}}
\put(1298,0){\line(0,1){1}}
\put(1312,0){\line(0,1){1}}
\put(1327,0){\line(0,1){1}}
\put(1342,0){\line(0,1){1}}
\put(1357,0){\line(0,1){1}}
\put(1371,0){\line(0,1){1}}
\put(1386,0){\line(0,1){1}}
\put(1401,0){\line(0,1){1}}
\put(1416,0){\line(0,1){1}}
\put(1430,0){\line(0,1){1}}
\put(1445,0){\line(0,1){1}}
\put(1475,0){\line(0,1){1}}
\end{picture}

{\smaller[2]
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&163&0.834&54.19&67.08&  0.0&  0.0&  0.0&  0.0&118.3&141.1&167.0 \end{tabular}
\begin{verbatim}

lowest :   0.00  24.46  24.51  24.52  59.00, highest: 194.00 194.90 195.00 200.00 200.10
\end{verbatim}
}
\smallskip\hrule\smallskip
}
\vbox{\noindent\textbf{agua}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){4}}
\put(59,0){\line(0,1){6}}
\put(189,0){\line(0,1){4}}
\put(213,0){\line(0,1){1}}
\put(225,0){\line(0,1){4}}
\put(237,0){\line(0,1){5}}
\put(249,0){\line(0,1){4}}
\put(272,0){\line(0,1){6}}
\put(284,0){\line(0,1){15}}
\put(296,0){\line(0,1){4}}
\put(343,0){\line(0,1){5}}
\put(367,0){\line(0,1){12}}
\put(379,0){\line(0,1){10}}
\put(391,0){\line(0,1){8}}
\put(403,0){\line(0,1){8}}
\put(414,0){\line(0,1){14}}
\put(426,0){\line(0,1){20}}
\put(438,0){\line(0,1){21}}
\put(450,0){\line(0,1){1}}
\put(462,0){\line(0,1){8}}
\put(474,0){\line(0,1){24}}
\put(485,0){\line(0,1){1}}
\put(497,0){\line(0,1){10}}
\put(509,0){\line(0,1){21}}
\put(521,0){\line(0,1){4}}
\put(533,0){\line(0,1){10}}
\put(545,0){\line(0,1){15}}
\put(568,0){\line(0,1){19}}
\put(580,0){\line(0,1){3}}
\put(592,0){\line(0,1){11}}
\put(604,0){\line(0,1){6}}
\put(616,0){\line(0,1){8}}
\put(628,0){\line(0,1){23}}
\put(639,0){\line(0,1){8}}
\put(651,0){\line(0,1){2}}
\put(663,0){\line(0,1){15}}
\put(675,0){\line(0,1){12}}
\put(687,0){\line(0,1){15}}
\put(699,0){\line(0,1){10}}
\put(710,0){\line(0,1){16}}
\put(722,0){\line(0,1){6}}
\put(734,0){\line(0,1){9}}
\put(746,0){\line(0,1){12}}
\put(758,0){\line(0,1){61}}
\put(770,0){\line(0,1){10}}
\put(781,0){\line(0,1){10}}
\put(793,0){\line(0,1){13}}
\put(805,0){\line(0,1){3}}
\put(817,0){\line(0,1){10}}
\put(829,0){\line(0,1){100}}
\put(841,0){\line(0,1){20}}
\put(852,0){\line(0,1){6}}
\put(864,0){\line(0,1){15}}
\put(876,0){\line(0,1){6}}
\put(888,0){\line(0,1){6}}
\put(900,0){\line(0,1){7}}
\put(912,0){\line(0,1){4}}
\put(924,0){\line(0,1){12}}
\put(935,0){\line(0,1){7}}
\put(947,0){\line(0,1){5}}
\put(959,0){\line(0,1){41}}
\put(995,0){\line(0,1){3}}
\put(1042,0){\line(0,1){8}}
\put(1054,0){\line(0,1){1}}
\put(1066,0){\line(0,1){1}}
\put(1077,0){\line(0,1){2}}
\put(1089,0){\line(0,1){1}}
\put(1101,0){\line(0,1){4}}
\put(1113,0){\line(0,1){1}}
\put(1137,0){\line(0,1){1}}
\put(1160,0){\line(0,1){3}}
\put(1172,0){\line(0,1){2}}
\put(1255,0){\line(0,1){44}}
\put(1362,0){\line(0,1){2}}
\put(1480,0){\line(0,1){2}}
\end{picture}

{\smaller[2]
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&205&0.998&181.6&23.82&146.1&154.6&164.9&185.0&192.0&203.5&228.0 \end{tabular}
\begin{verbatim}

lowest : 121.75 126.60 127.00 127.30 137.80, highest: 228.00 236.70 237.00 246.90 247.00
\end{verbatim}
}
\smallskip\hrule\smallskip
}
\vbox{\noindent\textbf{super\_plastificante}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){100}}
\put(69,0){\line(0,1){1}}
\put(91,0){\line(0,1){1}}
\put(114,0){\line(0,1){1}}
\put(137,0){\line(0,1){2}}
\put(160,0){\line(0,1){2}}
\put(183,0){\line(0,1){4}}
\put(206,0){\line(0,1){6}}
\put(229,0){\line(0,1){2}}
\put(251,0){\line(0,1){7}}
\put(274,0){\line(0,1){10}}
\put(297,0){\line(0,1){7}}
\put(320,0){\line(0,1){7}}
\put(343,0){\line(0,1){5}}
\put(366,0){\line(0,1){16}}
\put(388,0){\line(0,1){5}}
\put(411,0){\line(0,1){10}}
\put(434,0){\line(0,1){8}}
\put(457,0){\line(0,1){12}}
\put(480,0){\line(0,1){5}}
\put(503,0){\line(0,1){12}}
\put(526,0){\line(0,1){16}}
\put(548,0){\line(0,1){7}}
\put(571,0){\line(0,1){2}}
\put(594,0){\line(0,1){3}}
\put(640,0){\line(0,1){2}}
\put(663,0){\line(0,1){2}}
\put(686,0){\line(0,1){1}}
\put(708,0){\line(0,1){1}}
\put(731,0){\line(0,1){2}}
\put(754,0){\line(0,1){4}}
\put(823,0){\line(0,1){1}}
\put(846,0){\line(0,1){2}}
\put(868,0){\line(0,1){1}}
\put(914,0){\line(0,1){1}}
\put(960,0){\line(0,1){1}}
\put(1005,0){\line(0,1){2}}
\put(1074,0){\line(0,1){1}}
\put(1280,0){\line(0,1){1}}
\put(1462,0){\line(0,1){1}}
\end{picture}

{\smaller[2]
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&155&0.95&6.203&6.426& 0.00& 0.00& 0.00& 6.35&10.16&12.21&16.05 \end{tabular}
\begin{verbatim}

lowest :  0.00  1.72  1.90  2.00  2.20, highest: 22.00 22.10 23.40 28.20 32.20
\end{verbatim}
}
\smallskip\hrule\smallskip
}
\vbox{\noindent\textbf{agregador\_grosso}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){9}}
\put(43,0){\line(0,1){3}}
\put(64,0){\line(0,1){3}}
\put(86,0){\line(0,1){27}}
\put(107,0){\line(0,1){13}}
\put(128,0){\line(0,1){4}}
\put(150,0){\line(0,1){7}}
\put(171,0){\line(0,1){13}}
\put(193,0){\line(0,1){6}}
\put(214,0){\line(0,1){72}}
\put(236,0){\line(0,1){6}}
\put(257,0){\line(0,1){10}}
\put(278,0){\line(0,1){12}}
\put(300,0){\line(0,1){16}}
\put(321,0){\line(0,1){6}}
\put(343,0){\line(0,1){27}}
\put(364,0){\line(0,1){16}}
\put(385,0){\line(0,1){15}}
\put(407,0){\line(0,1){9}}
\put(428,0){\line(0,1){4}}
\put(450,0){\line(0,1){7}}
\put(471,0){\line(0,1){12}}
\put(493,0){\line(0,1){18}}
\put(514,0){\line(0,1){4}}
\put(535,0){\line(0,1){27}}
\put(557,0){\line(0,1){100}}
\put(578,0){\line(0,1){28}}
\put(600,0){\line(0,1){58}}
\put(621,0){\line(0,1){81}}
\put(642,0){\line(0,1){21}}
\put(664,0){\line(0,1){27}}
\put(685,0){\line(0,1){27}}
\put(707,0){\line(0,1){66}}
\put(728,0){\line(0,1){61}}
\put(750,0){\line(0,1){49}}
\put(771,0){\line(0,1){28}}
\put(792,0){\line(0,1){6}}
\put(814,0){\line(0,1){24}}
\put(835,0){\line(0,1){1}}
\put(857,0){\line(0,1){40}}
\put(878,0){\line(0,1){97}}
\put(899,0){\line(0,1){21}}
\put(921,0){\line(0,1){10}}
\put(942,0){\line(0,1){4}}
\put(964,0){\line(0,1){27}}
\put(985,0){\line(0,1){34}}
\put(1006,0){\line(0,1){1}}
\put(1028,0){\line(0,1){18}}
\put(1049,0){\line(0,1){55}}
\put(1071,0){\line(0,1){16}}
\put(1092,0){\line(0,1){42}}
\put(1114,0){\line(0,1){40}}
\put(1135,0){\line(0,1){18}}
\put(1156,0){\line(0,1){12}}
\put(1178,0){\line(0,1){19}}
\put(1199,0){\line(0,1){22}}
\put(1221,0){\line(0,1){22}}
\put(1242,0){\line(0,1){24}}
\put(1285,0){\line(0,1){4}}
\put(1306,0){\line(0,1){7}}
\put(1328,0){\line(0,1){10}}
\put(1349,0){\line(0,1){3}}
\put(1371,0){\line(0,1){10}}
\put(1392,0){\line(0,1){39}}
\put(1413,0){\line(0,1){1}}
\put(1435,0){\line(0,1){7}}
\put(1478,0){\line(0,1){1}}
\end{picture}

{\smaller[2]
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&284&1&972.9&88.55& 842.0& 852.1& 932.0& 968.0&1029.4&1076.5&1104.0 \end{tabular}
\begin{verbatim}

lowest :  801.0  801.1  801.4  811.0  814.0, highest: 1124.4 1125.0 1130.0 1134.3 1145.0
\end{verbatim}
}
\smallskip\hrule\smallskip
}
\vbox{\noindent\textbf{agregador\_fino}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){42}}
\put(37,0){\line(0,1){7}}
\put(55,0){\line(0,1){8}}
\put(74,0){\line(0,1){35}}
\put(111,0){\line(0,1){3}}
\put(129,0){\line(0,1){13}}
\put(148,0){\line(0,1){4}}
\put(166,0){\line(0,1){4}}
\put(185,0){\line(0,1){4}}
\put(203,0){\line(0,1){4}}
\put(222,0){\line(0,1){8}}
\put(240,0){\line(0,1){11}}
\put(258,0){\line(0,1){6}}
\put(277,0){\line(0,1){34}}
\put(295,0){\line(0,1){8}}
\put(314,0){\line(0,1){7}}
\put(332,0){\line(0,1){4}}
\put(351,0){\line(0,1){20}}
\put(369,0){\line(0,1){27}}
\put(388,0){\line(0,1){17}}
\put(406,0){\line(0,1){10}}
\put(425,0){\line(0,1){39}}
\put(443,0){\line(0,1){14}}
\put(462,0){\line(0,1){18}}
\put(480,0){\line(0,1){6}}
\put(498,0){\line(0,1){8}}
\put(517,0){\line(0,1){21}}
\put(535,0){\line(0,1){11}}
\put(554,0){\line(0,1){42}}
\put(572,0){\line(0,1){31}}
\put(591,0){\line(0,1){76}}
\put(609,0){\line(0,1){61}}
\put(628,0){\line(0,1){21}}
\put(646,0){\line(0,1){32}}
\put(665,0){\line(0,1){32}}
\put(683,0){\line(0,1){100}}
\put(702,0){\line(0,1){45}}
\put(720,0){\line(0,1){18}}
\put(738,0){\line(0,1){37}}
\put(757,0){\line(0,1){68}}
\put(775,0){\line(0,1){65}}
\put(794,0){\line(0,1){20}}
\put(812,0){\line(0,1){18}}
\put(831,0){\line(0,1){21}}
\put(849,0){\line(0,1){27}}
\put(868,0){\line(0,1){18}}
\put(886,0){\line(0,1){3}}
\put(905,0){\line(0,1){13}}
\put(923,0){\line(0,1){38}}
\put(942,0){\line(0,1){28}}
\put(960,0){\line(0,1){31}}
\put(978,0){\line(0,1){14}}
\put(997,0){\line(0,1){13}}
\put(1015,0){\line(0,1){21}}
\put(1034,0){\line(0,1){7}}
\put(1052,0){\line(0,1){10}}
\put(1071,0){\line(0,1){34}}
\put(1089,0){\line(0,1){20}}
\put(1108,0){\line(0,1){18}}
\put(1126,0){\line(0,1){21}}
\put(1145,0){\line(0,1){21}}
\put(1218,0){\line(0,1){7}}
\put(1274,0){\line(0,1){6}}
\put(1292,0){\line(0,1){11}}
\put(1477,0){\line(0,1){7}}
\end{picture}

{\smaller
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&304&1&773.6&89.87&613.0&664.1&730.9&779.5&824.0&880.8&898.1 \end{tabular}
\begin{verbatim}

lowest : 594.0 605.0 611.8 612.0 613.0, highest: 925.7 942.0 943.1 945.0 992.6
\end{verbatim}
}
\smallskip\hrule\smallskip
}
\vbox{\noindent\textbf{idade}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){1}}
\put(8,0){\line(0,1){32}}
\put(23,0){\line(0,1){30}}
\put(50,0){\line(0,1){15}}
\put(103,0){\line(0,1){100}}
\put(210,0){\line(0,1){21}}
\put(341,0){\line(0,1){13}}
\put(344,0){\line(0,1){5}}
\put(379,0){\line(0,1){12}}
\put(455,0){\line(0,1){1}}
\put(685,0){\line(0,1){6}}
\put(1029,0){\line(0,1){3}}
\put(1374,0){\line(0,1){1}}
\put(1393,0){\line(0,1){3}}
\end{picture}

{\smaller
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&14&0.925&45.66&50.89&  3&  3&  7& 28& 56&100&180 \end{tabular}
\begin{verbatim}

lowest :   1   3   7  14  28, highest: 120 180 270 360 365
                                                                                              
Value          1     3     7    14    28    56    90    91   100   120   180   270   360   365
Frequency      2   134   126    62   425    91    54    22    52     3    26    13     6    14
Proportion 0.002 0.130 0.122 0.060 0.413 0.088 0.052 0.021 0.050 0.003 0.025 0.013 0.006 0.014
\end{verbatim}
}
\smallskip\hrule\smallskip
}
\vbox{\noindent\textbf{forca\_compressiva}\setlength{\unitlength}{0.001in}\hfill\begin{picture}(1.5,.1)(1500,0)\linethickness{0.6pt}
\put(0,0){\line(0,1){2}}
\put(18,0){\line(0,1){2}}
\put(55,0){\line(0,1){10}}
\put(73,0){\line(0,1){8}}
\put(91,0){\line(0,1){15}}
\put(110,0){\line(0,1){25}}
\put(128,0){\line(0,1){15}}
\put(146,0){\line(0,1){40}}
\put(165,0){\line(0,1){30}}
\put(183,0){\line(0,1){38}}
\put(201,0){\line(0,1){48}}
\put(219,0){\line(0,1){35}}
\put(238,0){\line(0,1){55}}
\put(256,0){\line(0,1){40}}
\put(274,0){\line(0,1){30}}
\put(293,0){\line(0,1){55}}
\put(311,0){\line(0,1){22}}
\put(329,0){\line(0,1){30}}
\put(347,0){\line(0,1){38}}
\put(366,0){\line(0,1){50}}
\put(384,0){\line(0,1){38}}
\put(402,0){\line(0,1){92}}
\put(421,0){\line(0,1){58}}
\put(439,0){\line(0,1){60}}
\put(457,0){\line(0,1){38}}
\put(475,0){\line(0,1){42}}
\put(494,0){\line(0,1){45}}
\put(512,0){\line(0,1){60}}
\put(530,0){\line(0,1){50}}
\put(549,0){\line(0,1){58}}
\put(567,0){\line(0,1){100}}
\put(585,0){\line(0,1){62}}
\put(603,0){\line(0,1){52}}
\put(622,0){\line(0,1){42}}
\put(640,0){\line(0,1){72}}
\put(658,0){\line(0,1){52}}
\put(677,0){\line(0,1){90}}
\put(695,0){\line(0,1){50}}
\put(713,0){\line(0,1){70}}
\put(731,0){\line(0,1){58}}
\put(750,0){\line(0,1){35}}
\put(768,0){\line(0,1){65}}
\put(786,0){\line(0,1){38}}
\put(805,0){\line(0,1){32}}
\put(823,0){\line(0,1){30}}
\put(841,0){\line(0,1){25}}
\put(859,0){\line(0,1){32}}
\put(878,0){\line(0,1){22}}
\put(896,0){\line(0,1){42}}
\put(914,0){\line(0,1){35}}
\put(932,0){\line(0,1){30}}
\put(951,0){\line(0,1){32}}
\put(969,0){\line(0,1){30}}
\put(987,0){\line(0,1){45}}
\put(1006,0){\line(0,1){30}}
\put(1024,0){\line(0,1){5}}
\put(1042,0){\line(0,1){22}}
\put(1060,0){\line(0,1){25}}
\put(1079,0){\line(0,1){15}}
\put(1097,0){\line(0,1){15}}
\put(1115,0){\line(0,1){10}}
\put(1134,0){\line(0,1){12}}
\put(1152,0){\line(0,1){15}}
\put(1170,0){\line(0,1){15}}
\put(1188,0){\line(0,1){20}}
\put(1207,0){\line(0,1){18}}
\put(1225,0){\line(0,1){5}}
\put(1243,0){\line(0,1){5}}
\put(1262,0){\line(0,1){12}}
\put(1280,0){\line(0,1){12}}
\put(1298,0){\line(0,1){5}}
\put(1316,0){\line(0,1){12}}
\put(1335,0){\line(0,1){8}}
\put(1353,0){\line(0,1){2}}
\put(1371,0){\line(0,1){12}}
\put(1408,0){\line(0,1){15}}
\put(1426,0){\line(0,1){5}}
\put(1463,0){\line(0,1){2}}
\put(1481,0){\line(0,1){2}}
\end{picture}

{\smaller
\begin{tabular}{ rrrrrrrrrrrrr }
n&missing&distinct&Info&Mean&Gmd&.05&.10&.25&.50&.75&.90&.95 \\
1030&0&938&1&35.82&18.92&10.96&14.20&23.71&34.44&46.14&58.82&66.80 \end{tabular}
\begin{verbatim}

lowest :  2.331808  3.319827  4.565021  4.782206  4.827711
highest: 79.400056 79.986111 80.199848 81.751169 82.599225
\end{verbatim}
}
\smallskip\hrule\smallskip
}
}\end{spacing}

## Preparação dos dados
```{r, echo=TRUE, cache=TRUE}
## Separando o conjunto de dados em treino e teste
set.seed(2)
inTrain <- createDataPartition(dados$forca_compressiva, p = 7/10)[[1]]
treino <- dados[inTrain,]
teste <- dados[-inTrain,]

## Mantendo casos completos em treino e teste
treino <- treino[complete.cases(treino),]
teste <- teste[complete.cases(teste),]

## Separando a variavel resposta, categóricas e numericas
resposta <- treino$forca_compressiva
resposta_teste <- teste$forca_compressiva

## Criando dataset normalizado para avaliar diferença de resultado
normalized_train <- treino
normalized_teste <- teste

maxTrainFeatures <- apply(normalized_train[,1:8], 2, max) #max of each feature
minTrainFeatures <- apply(normalized_train[,1:8], 2, min) #min of each feature


minMaxDiffTrain <- (maxTrainFeatures - minTrainFeatures)
minMaxDiffTrain

normalized_train[,1:8] <- sweep(normalized_train[,1:8], 2, minTrainFeatures, "-")
normalized_train[,1:8] <- sweep(normalized_train[,1:8], 2, minMaxDiffTrain, "/")

normalized_teste[,1:8] <- sweep(normalized_teste[,1:8], 2, minTrainFeatures, "-")
normalized_teste[,1:8] <- sweep(normalized_teste[,1:8], 2, minMaxDiffTrain, "/")

## Retendo as numéricas
Ind_numericas <- colnames(treino[,-ncol(treino)])[sapply(treino[,-ncol(treino)], is.numeric)]
Ind_categoricas <- colnames(treino[,-ncol(treino)])[sapply(treino[,-ncol(treino)], function(x) !is.numeric(x))]
numericas <- treino[,Ind_numericas]
categorias <- treino[,Ind_categoricas]
```

## Redução de dimensionalidade

### Estrutura de correlações

Como são todas variáveis numéricas inicialmente veremos na matriz de correlação se há algumas relação mais forte entre pares de variáveis. Se houver poderemos escolher somente uma das variáveis pois adiconar outra variável fortemente correlacionanda não adicionaria novas informações e traria dificuldades no processo de estimação em virtude de possível multicolinearidade.

```{r echo=TRUE}
## Adicionando pacote corrplot
require(corrplot)
require(GGally)

## Analisando as correlações
M <- cor(numericas, use = 'complete.obs')
corrplot(M, method='number', diag = T, number.cex = 0.8)
summary(M[upper.tri(M)])

## Imprimindo as correlações na forma de circulos
M <- cor(numericas, use = 'complete.obs')
summary(M[upper.tri(M)])
corrplot(M, method='circle')

## Visualizando as correlações
ggpairs(numericas)
```

Como as maiores correlações foram de $0,65$, não podemos afirmar que há pares de variáveis redundantes. Portanto optamos por não retirar nenhuma variável nessa etapa. Como não temos variáveis altamente correlacionadas, provavelmente não teremos multicolinearidade, e ainda que tívessemos, provavelmente não poderíamos remover covariáveis da análise, entretando, vamos verificar mesmo assim.

Dependendo do objetivo da análise, se queremos acertar o valor da força compressevia de uma certa batelagem de cimento, ou do cimento utilizado em uma certa obra, ou se apenas queremos entender como essas variáveis influenciam na força compressiva, temos mais ou menos liberdades para modificar as covariáveis. 


### Análise de redundância

Na análise de redundância utilizamos regressões de cada variável tendo as outras como suas preditoras, inclusive com componentes não lineares via *splines* cúbicos. Essa análise é superior ao correlograma no sentido de que considera não somente as relações lineares dois a dois, mas também a capacidade das preditoras fornecerem informações sobre as outras preditoras de forma conjunta.

```{r, echo=TRUE, cache=TRUE}
redun(~ ., r2 = .8, type = "adjusted", data = numericas)
```

Como resultado dessa análise as variáveis cimento, considerando como critério um $R^2$ água e cinza poderiam ser facilmente preditas a partir das outras, portanto seriam excluídas. Entretando são variáveis fundamentais na produção do concreto e o $R^2 < 0,9$. Portanto não excluimos nenhuma variável.

### Estrutura das variáveis com PCA

```{r, echo=TRUE, cache=TRUE}
# Calculando o PCA
prin.raw <- princomp (~ ., cor = TRUE , data = numericas)
plot (prin.raw, type = 'lines' , main = ' ' , ylim = c (0 ,3))

# Adicionando a variância cumulativa explicada
addscree <- function (x , npcs = min (10 , length (x$sdev)) ,
                      plotv = FALSE ,
                      col =1 , offset = .8 , adj =0 , pr = FALSE) {
                        vars <- x$sdev^2
                        cumv <- cumsum(vars)/sum(vars)
                        if (pr) print(cumv)
                        text (1: npcs , vars [1: npcs ] + offset*par ('cxy')[2] ,
                        as.character(round(cumv [1: npcs ], 2)),
                        srt =45 , adj = adj , cex = .65 , xpd = NA , col = col)
                        if ( plotv ) lines (1: npcs , vars [1: npcs ], type = ' b ' , col = col )
}
addscree (prin.raw)
```
Não parece haver uma estrutura onde as primeiras componente dominam as outras. Portanto com base nessa análise ainda não teríamos indicação de eliminar variáveis.

## Modelagem

### Modelos lineares com polinômios

```{r echo=TRUE, cache=TRUE}
## Modelo linear sem interações e sem termos polinomiais
f1 <-
  formula(
    "forca_compressiva ~ cimento + escoria + cinza + agua +
      super_plastificante + agregador_grosso +
      agregador_fino + idade"
  )

## Transformando a variável resposta pelo log()
f1_log <-
  formula(
    "log(forca_compressiva) ~ cimento + escoria + cinza + agua +
      super_plastificante + agregador_grosso +
      agregador_fino + idade"
  )

## Modelo com polinômios
f2 <- formula(
  "forca_compressiva ~ cimento + escoria + cinza + agua +
    super_plastificante + agregador_grosso + agregador_fino + idade +
    I(cimento ^ 2) + I(escoria ^ 2) + I(cinza ^ 2) +
    I(agua ^ 2) + I(super_plastificante ^2) +
    I(agregador_grosso ^ 2) + I(agregador_fino ^ 2) + I(idade ^ 2)"
  )

## Modelo com polinômio na variável transformada por log
f2_log <- formula(
  "log(forca_compressiva) ~ cimento + escoria + cinza + agua +
    super_plastificante + agregador_grosso + agregador_fino + idade +
    I(cimento ^ 2) + I(escoria ^ 2) + I(cinza ^ 2) +
    I(agua ^ 2) + I(super_plastificante ^2) +
    I(agregador_grosso ^ 2) + I(agregador_fino ^ 2) + I(idade ^ 2)"
  )

## Salvando as fórmulas
formulas <- c(f1, f2)

## Criando os modelos
for (f in formulas) {
  ##model
  model <- lm(formula = f, data=treino)
  
  model_norm <- lm(formula = f, data=normalized_train)
  
  ##predicao treino
  treinoPred     <- predict(model, treino)
  treinoPredNorm <- predict(model_norm, normalized_train)
  
  ##predicao teste
  testePred     <- predict(model, teste)
  testePredNorm <- predict(model_norm, normalized_teste)
  
  
  mae_treino  <- round(MAE(treino$forca_compressiva, treinoPred), 3)
  mae_teste   <- round(MAE(teste$forca_compressiva, testePred), 3)
  
  
  rmse_treino <- round(RMSE(treino$forca_compressiva, treinoPred), 3)
  rmse_teste  <- round(RMSE(teste$forca_compressiva, testePred), 3)
  
  
  mae_norm_treino  <- round(MAE(normalized_train$forca_compressiva, treinoPredNorm), 3)
  mae_norm_teste   <- round(MAE(normalized_teste$forca_compressiva, testePredNorm), 3)
  
  rmse_norm_treino <- round(RMSE(normalized_train$forca_compressiva, treinoPredNorm), 3)
  rmse_norm_teste  <- round(RMSE(normalized_teste$forca_compressiva, testePredNorm), 3)
  
  print(f)
  
  print(paste0('DATASET NÂO NORMALIZADO (TREINO -- TESTE): \n'))
  print(paste0('MAE :', mae_treino, ' --  ', mae_teste))
  print(paste0('RMSE :', rmse_treino, ' -- ', rmse_teste))

  print(paste0('DATASET NORMALIZADO (TREINO -- TESTE): \n'))
  print(paste0('MAE :', mae_norm_teste, ' -- ', mae_norm_teste))
  print(paste0('RMSE :', rmse_norm_treino, ' -- ', rmse_norm_teste))
  
  print('MUDANDO DE MODELO')
  print('')
  print('')
  print('')

}

formulas_log <- c(f1_log, f2_log)

for (f in formulas_log) {
  ##model
  model <- lm(formula = f, data=treino)
  
  model_norm <- lm(formula = f, data=normalized_train)
  
  ##predicao treino
  treinoPred     <- predict(model, treino)
  treinoPredNorm <- predict(model_norm, normalized_train)
  
  ##predicao teste
  testePred     <- predict(model, teste)
  testePredNorm <- predict(model_norm, normalized_teste)
  
  
  mae_treino  <- round(MAE(log(treino$forca_compressiva), treinoPred), 3)
  mae_teste   <- round(MAE(log(teste$forca_compressiva), testePred), 3)
  
  
  rmse_treino <- round(RMSE(log(treino$forca_compressiva), treinoPred), 3)
  rmse_teste  <- round(RMSE(log(teste$forca_compressiva), testePred), 3)
  
  
  mae_norm_treino  <- round(MAE(log(normalized_train$forca_compressiva), treinoPredNorm), 3)
  mae_norm_teste   <- round(MAE(log(normalized_teste$forca_compressiva), testePredNorm), 3)
  
  rmse_norm_treino <- round(RMSE(log(normalized_train$forca_compressiva), treinoPredNorm), 3)
  rmse_norm_teste  <- round(RMSE(log(normalized_teste$forca_compressiva), testePredNorm), 3)
  
  print(f)
  
  print(paste0('DATASET NÂO NORMALIZADO (TREINO -- TESTE): \n'))
  print(paste0('MAE :', mae_treino, ' --  ', mae_teste))
  print(paste0('RMSE :', rmse_treino, ' -- ', rmse_teste))
  
  print(paste0('DATASET NORMALIZADO (TREINO -- TESTE): \n'))
  print(paste0('MAE :', mae_norm_teste, ' -- ', mae_norm_teste))
  print(paste0('RMSE :', rmse_norm_treino, ' -- ', rmse_norm_teste))
  
  print('MUDANDO DE MODELO')
  print('')
  print('')
  print('')
  
}

```

Aplicando o $log()$ na variável resposta, analisando $MAE$ e $RMSE$, temos um melhor desempenho. Notamos também que normalização fez pouca diferença, entretanto os modelos com termos quadráticos tiveram um melhor resultado. Isso já era esperado pois no artigo original dos dados o modelo original foi criado com redes neurais. 

### Análise de Resíduos
```{r echo=TRUE, cache=TRUE}
### Modelo final
final_model <- lm(formula = f1, data = treino)

## Obtendo os resíduos
e <- resid(final_model)

## Gráfico de resíduos
#plot(log(treino$forca_compressiva), e,
#     ylab="Resíduos", xlab="Valor Observado", 
#     main="Análise de Resíduos") 
#abline(0, 0)

## Plot automatico
par(mfrow=c(2,2))
plot(final_model)
```

## Detecção de outliers e medidas influentes

Com base nos gráficos do modelo final podemos ainda nos aprofundar nas medidas influentes. Vemos que estão indicadas nos gráficos algumas observações influentes baseadas em *Leverage* e *Cook's distance*. Vamos então analisar alguns gráficos.

```{r, eval=TRUE, echo=TRUE}
## Carregando os pacotes necessários
require(olsrr)

## Distância de Cook
ols_plot_cooksd_bar(final_model)
```

De acordo com esse critério há muitas observações que podem ser classificadas como outliers. Essas observações, nesse critério, tem influência forte sobre os valores ajustados.

```{r, echo=TRUE, cache=TRUE}
## Dfbeta
ols_plot_dfbetas(model)
```

Com os dfbetas conseguimos entender quais medidas são influentes em um dado $\beta_i$. Aqui vemos que em todas as variáveis há medidas muito influentes.

```{r, echo=TRUE, eval=TRUE}
## Comparando resíduo e leverage
ols_plot_resid_lev(model)
```

Aqui podemos observar que outliers e medidas influentes são bastante diferentes. Há medidas que são fortemente influentes e ainda são outliers, o que representa o pior caso. Entretanto simples outliers podem não ser tão prejudiciais para o modelo. Neste caso em específico decidimos não remover nenhuma observação devido a natureza não linear dos dados. No fim das contas estamos ajustando um modelo de regressão linear a um conjunto de dados ajustado originalmente para uma rede neural. 

### Relaxando a linearidade com splines
```{r, echo=TRUE, eval=TRUE, cache=TRUE}
## Carregando os pacotes
require(ggplot2)
require(rms)

## Definindo as estatísticas de resumo para os plots
d <- datadist(dados)
options(datadist = "d")

## Ajustando um modelo com splines cúbicos
mod1 <- ols(forca_compressiva ~ rcs(cimento, 5) + rcs(escoria, 5) + rcs(cinza, 5) + 
                  rcs(agua, 5) + rcs(super_plastificante, 5) + rcs(agregador_grosso, 5) +
                  rcs(agregador_fino, 5) + rcs(idade, 5),
              data = dados, x = TRUE, y = TRUE)

## Avaliando o modelo
mod1
anova(mod1)
summary(mod1)
plot(summary(mod1))
```

De acordo com o modelo é possível entender o impacto de cada variável na força compressiva. Nesse gráfico está registrada a variação interquartílica e o impacto na variável resposta. É fácil observar que a idade, cimento, cinza e escória, componentes fundamentais na produção do concreto, são as variáveis mais importantes. 

```{r, cache=TRUE, echo=TRUE}
## Fazendo os plots dos resíduos
both <- data.frame(residuos = resid(mod1), ajustado = fitted (mod1))
both$cimento <- dados$cimento
both$dia <- dados$idade

yl <- ylab ('Residuals')
p1 <-  ggplot(both , aes( x = ajustado , y = residuos)) + geom_point() + yl
p2 <- ggplot(both , aes( x = cimento , y = residuos )) + geom_point() + yl
p3 <- ggplot(both , aes( x = dia , y = residuos)) + yl + ylim ( -100 , 100) +
stat_summary(fun.data = "mean_sdl", geom = 'smooth')
p4 <- ggplot(both , aes(sample = residuos)) + stat_qq() +
geom_abline(intercept = mean(resid(mod1)), slope = sd(resid(mod1))) + yl
gridExtra::grid.arrange( p1 , p2 , p3 , p4 , ncol =2)
```

Aqui é importante observar que não há grandes padrões nos resíduos em comparação com as variáveis mais importantes como o número de dias e a quantidade de cimento. Entretanto, especialmente nos valores mais baixos de força do concreto vemos algumas distorções. É possível que serja necessário utilizar modelos com ainda mais linearidade ou apelar para modelos de aprendizado de máquina.

### Ouliers e medidas influentes

```{r cache=TRUE, eval=TRUE}
mod1_plot <- mod1
class(mod1_plot) <- "lm"
## Comparando resíduo e leverage
ols_plot_resid_lev(model)
```
Da mesma forma que o modelo anterior, sem splines e com polinômios, há também medidas influentes e outliers.

## Comparando os modelos com splines e com polinômios 

```{r, eval=TRUE, cache=TRUE}
##predicao teste
  testePred_pol    <- predict(final_model, teste)
  testePred_spline <- predict(mod1, teste)
  
## RMSE
  rmse_pol <- round(RMSE(teste$forca_compressiva, testePred_pol), 3)
  rmse_spline <- round(RMSE(teste$forca_compressiva, testePred_spline), 3)
  
## MAE
  mae_pol <- round(MAE(teste$forca_compressiva, testePred_pol), 3)
  mae_spline <- round(MAE(teste$forca_compressiva, testePred_spline), 3)
  
## Data.frame com os resultados finais
resultados <- data.frame(MAE = c(mae_spline, mae_pol), RMSE = c(rmse_spline, rmse_pol), row.names = c("Spline", "Polinômio"))

## Mostrando o resultado
kable(resultados)
```

## Conclusão

O modelo com splines é levemente superior. A vantagem se deve a forma mais inteligente de incluir a não-linearidade por meio dos splines que se comportam melhor que os polinômios, especialmente nos limites do range dos dados.



